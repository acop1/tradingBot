import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor
import numpy as np
import matplotlib.pyplot as plt

#this will be the msft randomForest model which predicts if the stock is going up or 
#going down for the particular day. This is completely based on the ipynb file that is in this folder. 
#After the model creation, it will be condensed into a function will be imported into trading_bot.py folder
#Make the model simple to implement in trading_bot.py

#this is a py file of the visualized ipynb file 
#   
#----------------------------------------------------------------------------------------------------------------------------------------------------------------------
#create a test observation that has yesterdays values and use that to predict todays price.
#updated version of the rftrader with proper implementation of machine learning
def get_prediction_score(target, predictions):
    combined = pd.concat([target, predictions], axis=1)
    combined['check'] = (combined['target'] == combined[0]).astype(int)
    check_sum = combined['check'].sum()
    score = check_sum / len(combined[0])
    return score*100

def predict_rf(test, symbol):
    #initializing the model
    model_create(symbol, "rf")
    model = models[symbol+"_rf"]
    predictors = ['close','volume','percentDif','totalRevenue','grossProfit']
    preds = model.predict(test[predictors])
    return preds[0]

#
def prep_data(symbol:str, today:str):
    #prepping the data for modelling
    path = "/Users/acop/Desktop/Projects/TradingBot/bot/StockCSV/FinHistData/Adjusted/" + symbol + "adjFinHistData.csv"
    data = pd.read_csv(path)
    data = data.drop(columns={'Unnamed: 0'})
    data = data.copy().loc[(data['date'] < today)]
    test = data.copy().iloc[-1:]
    test = test.reset_index()
    del test['index']
    test.at[0, 'date'] = today
    data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')
    test['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d')
    data.set_index(pd.DatetimeIndex(data.pop('date')),inplace=True)
    test.set_index(pd.DatetimeIndex(test.pop('date')),inplace=True)
    data['tomorrow'] = data['close'].shift(-1)
    data['target'] = (data['tomorrow'] > data['close']).astype(int) 
    return data, test

#----------------------------------------------------------------------------------------------------------------------------------------------------------------------
#this is the updated version of the moving averages predictor. It uses a singular model that is trained once and then predicts whether the stock is going up or down 
#for the day
def horizon_prep(symbol:str, today:str):
    data, test = prep_data(symbol, today)

    test['tomorrow'] = test['close']
    test['target'] = 0

    horizons = [2,5,21,130,260]
    new_predictors = []

    for horizon in horizons:
        rolling_averages =  data.rolling(horizon).mean()
        
        ratio_column = f"close_ratio_{horizon}"
        data[ratio_column] = data['close']/ rolling_averages['close']
        test[ratio_column] = test['close']/ rolling_averages['close']

        trend_column = f"trend_{horizon}"
        data[trend_column] = data.shift(1).rolling(horizon).sum()['target']
        test[trend_column] = test.shift(1).rolling(horizon).sum()['target']

        new_predictors += [ratio_column, trend_column]
        
    for x in new_predictors:
        test[x] = data[x].iloc[-1]
        

    data = data.dropna()
    return data, test, new_predictors

#this model will act similar to the horizons moving averages random forest model
def predict_rfma(test, symbol, new_predictors):
    model_create(symbol, "rfma")
    model = models[symbol+"_rfma"]
    pred = model.predict_proba(test[new_predictors])[:,1]
    pred[pred >= .6] = 1
    pred[pred < .6] = 0
    return int(pred[0])


#----------------------------------------------------------------------------------------------------------------------------------------------------------------------
#create a new parameter called difference and predict the change of the stock instead of the direction of the stock.

def predict_xgb_delta(test, symbol):
    model_create(symbol, "rf_delta")
    model = models[symbol+"_rf_delta"]
    predictors = ['close','volume','percentDif','totalRevenue','grossProfit']
    preds = model.predict(test[predictors])
    return preds[0]

def prep_data_delta(symbol:str, today:str):
    path = "/Users/acop/Desktop/Projects/TradingBot/bot/StockCSV/FinHistData/Adjusted/" + symbol + "adjFinHistData.csv"
    data = pd.read_csv(path)
    data = data.copy().loc[(data['date'] < today)]
    data['yesterday'] = data['close'].shift(1)
    data['delta'] = data.close.sub(data.yesterday)
    data = data.drop(columns={'Unnamed: 0','open','high','low','yesterday'})
    test = data.copy().iloc[-1:]
    test = test.reset_index()
    del test['index']
    test.at[0, 'date'] = today
    data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')
    test['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d')
    data.set_index(pd.DatetimeIndex(data.pop('date')),inplace=True)
    test.set_index(pd.DatetimeIndex(test.pop('date')),inplace=True)
    data.dropna(inplace=True)
    return data, test
#do some research to find out what magnitude of delta implies what type of behavior from the stock. Use data analysis and industry research to find this out

#----------------------------------------------------------------------------------------------------------------------------------------------------------------------
#training on a global dataset from everything before 2023-01-01
#can use the data from running prep_data, horizons instead of rewriting all of the code.
models = {}
def model_create(symbol: str, trader:str):
        if (symbol+"_"+trader) not in models.keys():
            predictors = ['close','volume','percentDif','totalRevenue','grossProfit']
            if trader != "rf_delta":
                train, test = prep_data(symbol, "2023-01-01")
                train.dropna(inplace=True)
                model = RandomForestClassifier(n_estimators=100, min_samples_split=100, random_state=1)

                if trader == "rf":
                    model.fit(train[predictors], train['target'])
                    models[symbol+"_"+trader] = model

                elif trader == "rfma":
                    train, test, new_predictors = horizon_prep(symbol, "2023-01-01")
                    
                    model.fit(train[new_predictors], train['target'])
                    models[symbol+"_"+trader] = model
            else:
                data, test = prep_data_delta(symbol, "2023-01-01")
                param_grid = {
                    'learning_rate': [0.01, 0.1, 0.2],
                    'max_depth': [3, 5, 7],
                    'subsample': [0.8, 0.9, 1.0]
                }

                X_train = data[predictors]
                y_train = data["delta"]

                grid_search = GridSearchCV(XGBRegressor(), param_grid, cv=3)
                grid_search.fit(X_train, y_train)
                best_params = grid_search.best_params_

                xgb_model = XGBRegressor(**best_params)
                xgb_model.fit(X_train, y_train)
                models[symbol+"_"+trader] = xgb_model

#find a way to implement this delta prediction into the trader and make more money than the direction trading strategy

"""data, test = prep_data_delta("CRM", "2024-08-31")
pred = predict_xgb_delta(test, "CRM")
print(pred, pred.dtype)
last_price = 250
per_chng = (((last_price - pred) - last_price) / last_price)*100
tf = per_chng > 1
print(per_chng, tf)"""



lst = [0.08696918896952892, 0.12680360479569192, 0.9445884663885925, 0.14776778866677245, 0.14257586897730476, 0.7501367101083279, 0.08092899895311137, 0.7398365434176758, -1.1385376974810368, 0.135894998702595, 0.13657055321771736, 0.1117784741679192, 0.09874279556236777, 0.12920172765656623, 1.0496292714823499, 0.07775520367091876, 0.12178477049842097, 0.6474682169044452, 0.11742020041726141, 0.09908786084045573, 0.06374892913481536, 0.10964163910051739, 0.11331097905160911, 0.06364647387748672, 0.06380979378206648, 0.06289769183186925, 0.08641139698292728, -0.2591415683645338, -0.2581868113630375, 0.1145234395593401, 0.11409613530401995, 0.06327360946710132, 0.09075736355852013, 0.0914723775180286, 0.06555458016925038, 0.06468998757176934, 0.06658327327512921, 0.0727827471461831, 0.07617200812303093, 0.07873748990740062, 0.6746643492985951, 2.6671643399184575, -1.005727647033313, 0.10444070498357524, 0.10510165205524565, 0.06761461482559229, 0.0839086139903349, 0.7413299878438314, 0.10753032344654782, 0.10510165205524565, 0.08050789151684623, 0.10355371875788663, 0.10364842980882963, 0.05627688216535669, 0.0684814532043906, 0.0639395443747574, 0.10329830103201776, 0.055929532488010775, 0.0775483465082662, 0.05406587136786898, 0.07584023079989848, 0.06498265003577304, 0.06018029469191838, 0.05400306874453419, 0.05511653407947304, 0.05511653407947304, 0.05655668967256758, 0.05618816311058909, 0.05614391064717978, 0.055734205218885474, 0.055734205218885474, 0.053710104043960386, 0.05447074602743884, 0.05422764990664845, 0.054268932028950036, 0.054268932028950036, 0.05512505822615064, 0.0555114104582576, 0.05506827889092445, 0.054512403487815844, 0.054512403487815844, 0.05399761327499786, 0.05505693446152528, 0.055719687084438516, 0.055130743901332546, 0.055130743901332546, 0.05433787822821521, 0.05217433143378077, 0.06324687635359616, 0.05256160839792797, 0.05256160839792797, 0.052952052687813535, 0.051780182137616315, 0.05103869981583661, 0.05020946315889418, 0.05020946315889418, 0.0514711048843677, 0.05189578703086387, 0.05043206959696596, 0.0502472154984748, 0.0502472154984748, 0.058453307830288494, 0.09252297103798018, 0.4634484116650113, 0.3640002847315922, 0.070070339093017, 0.06970256541758293, 0.060279657673184064, 0.0914382880973625, 0.05075042710862101, 0.06950203061380408, -0.20918477101292132, -0.81253672070384, 0.0902179158386364, 0.07013996472878706, 0.0289545791198624, 0.0919006473011386, 0.07115291020083031, 0.050796235683694874, 0.0030523849137742147, 0.05130563515920984, 0.050460631993416834, 0.050470157559771696, 0.050470157559771696, 0.050779347672976774, 0.050627877679553936, 0.05097300564254944, 0.05097300564254944, 0.04815188565692194, 0.04818660615378763, 0.08439995446461977, 0.0838708463144311, 0.046550316732599316, 0.047093624609995305, 0.045698811037969723, 0.04626831384257202, 0.0807024862455285, 0.04648959831051205, 0.04733755702265519, 0.04763702794078718, 0.04688917701379353, 0.04731242305937067, 0.04731242305937067, 0.0477583082789188, 0.04816923964870243, 0.04945702017737566, 0.049409026525929996, 0.049409026525929996, 0.05007074436250988, 0.050520235859999145, 0.05136232100877436, 0.07266136536400794, 0.05187312549425618, 0.050837291486387624, 0.05140676736258543, 0.05176263364620671, 0.05320499351380936, 0.05320499351380936, 0.05095357303036378, 0.05140676736258543, 0.05067346519015889, 0.05202962336248858, 0.05202962336248858, 0.051717570067316906, 0.050491605967296384, 0.046897401804463903, 0.08619746372470385, 0.6855254205281303, 0.048720132828837436, 0.0486536260797518, 0.04788020446324861, 0.04788020446324861, 0.047852350671074316, 0.04834346670357128, 0.0487312355329839, 0.048853693288076, 0.048853693288076, 0.08986022294564877, 0.049686839047629966, 0.050965717139574575, 0.05108503051042513, 0.05108503051042513, 0.05203975056160356, 0.05267553732726933, 0.053260649053532845, 0.05213871423809701, 0.05213871423809701, 0.052737892041517985, 0.0532288299059706, 0.05293370104662262, 0.053335035112178894, 0.053335035112178894, 0.05149589502431309, 0.051597779432839205, 0.05144881642309664, 0.05213108858465934, 0.05213108858465934, 0.051782688396372546, 0.051190192270517326, 0.051792723514054825, 0.05156543007303593, 0.05156543007303593, 0.05248420868627351, 0.052621101240180254, 0.054161724802911815, 0.054194665210485445, 0.054194665210485445, 0.0533510014425026, 0.05321823466485799, 0.05182786940987182, 0.05106551240549065, 0.05106551240549065, 0.05135491699067134, 0.05103869981583661, 0.05033710433851691, 0.050066054430505394, 0.05106307508635171, 0.05030158179892733, 0.048824692289578864, 0.048437633573806434, 0.04860276187008077, 0.04834565130868247, 0.048288885720526274, 0.04773485540811505, 0.04767100965643065, 0.047658261643472435, 0.047417328653737344, 0.04781811002295028, 0.04677226535527862, -0.750947673872536, 1.1547022819519044, 0.5397670188762929, -0.2401610780261906, 0.002353408450331302, 0.006170531912664672, 0.011132724537885998, 0.0023677172139286995, 0.002350791486529502, 0.01490361126814955, 0.01976588054707176, 0.019881371199308027, 0.23364531455438955, -0.2440216211513474, 0.008704348665162131, -0.04634789637060409, 0.01839445999071618, 0.00857748621275624, 0.008564262174527175, 0.008546266054793998, 0.008594316515126705, 0.008741766442407648, 0.015139365219740707, 0.0023505114528779725, 0.002356312597676068, 0.017014487898629124, -0.03040265543153061, 0.008651118618325779, -0.004618885968603305, -0.28300210794258107, 0.018228043508497993, 0.018323841366343924, 0.018007705336088663, 0.18931416174751786, -0.3441508530382781, 0.06268266108827668, 0.0628729558575316, 0.06480872135328011, 0.20002399449781, 0.18602438468009955, 0.16942697181768604, 0.18428890323252128, 0.1980876529368772, 0.1750454064486197, 0.1744033259595846, 0.18179919521334856, 0.1944064720814878, 0.17256465317121744, 0.19049514386761623, 0.19208685624197894, 0.17803556008682073, 0.18367459899500796, 0.17135632828810565, 0.17096901389017496, 0.19296269754465792, 0.19945977604988552, 0.17096901389017496, 0.17566194870327945, 0.18898711604340457, -0.27569890720138174, 0.14528345316985847, -0.6593309243520101, -1.8247891714984508, -0.26431405027779853, -0.26708192279026205, -0.2729206446661954, -0.2796338274587993, 0.05825006671301865, 0.17067347713465097, 0.17163036299533532, 0.17163036299533532, 0.17509788001664095, 0.17509788001664095, -0.6826305966464028, 0.17335252012440078, 0.1621190870303713, 0.16923334887519464, 0.14070869623437643, 0.16236161790091416, 0.18170277288436176, 0.16640984018643695, 0.16640984018643695, 0.17595717886916729, 0.16422564446099264, 0.16981528273283883, 0.17007205356230387, -0.2832502224958438, 0.18417084132358874, 0.1672740828078778, 0.16670435052738025, 0.16816435772239066, 0.16816435772239066, 0.18168335496656426, -1.9939940354251804, -0.3412326995366038, 0.2050082679775857, 0.018041409713889855, 0.01787999032002402, 0.0633207585314195, 0.16025041963073505, 0.18153800747611307, 0.19035367532209915, 0.18186204611830006, 0.18557339266486236, 0.018178839958207318, 0.009001486454793371, 0.009001486454793371, 0.18114936216597072, 0.18105737521044268, 0.18342561484712003, 0.18081474448506146, 0.18959528995428995, 0.1802532986575009, 0.17846197826097035, 0.17249308872828314, 0.16977522045876942, 0.17392332277651157, 0.17407493485991274, 0.17615098765338877, 0.17414173758679807, 0.18426513640249975, 0.17987037998232452, -0.3102435549693321, -0.3435929585655563, -0.21219365013513963, 0.9450091569447686, 0.6726383116621244, 0.30809467441937566, -0.1877802451911407, 0.07290917425161694, -0.6895186663731565, 0.056197680884444094, 0.06713538111927811, 0.05681315617343111, 0.0770354473935381, 0.08557572593429227, 0.07665776057537446, 0.07971035220606622, 0.055620544445780656, 0.3146818911726224, 0.49840557155329523, 0.05541323856816736, 0.04842859141680659, 0.03531982266221056, 0.7911281472832493, 0.7990467945864295, 0.014934348661882693, 0.014934348661882693, -0.004711713740233259, -0.035618759496710506, 0.015277982204733165, 0.01220566560521258, -0.005647297100289548, 0.002352473233323506, 0.002332450572659014, 0.0023304303286699798, 0.015221024401104328, 0.002380572362807301, 0.04771769631354643, 0.034894482141384256, 0.0023338299573651415, 0.015065670881864128, 0.005945383791464163, 0.03146341322243864, -0.004767974204093713, 0.014979232704424663, 0.01476656088238632, 0.01548823272605919, 0.0023888345757314376, 0.04921400753803432, 0.04444657801659674, 0.03739334025468608, 0.036572402093155244, 0.006171772545677894, 0.01696136010729748, 0.01680263172951868, 0.014881074734513926, 0.014662193250064065, -0.004729174554291063, 0.008596262686045492, 0.008723017427658087, -0.004733695753673751, 0.00869338519981274, 0.01464876226672235, -0.0047674233340409345, 0.008698697706169661, -0.00456758567942274]
lst.sort()
plt.plot(lst)
plt.show()